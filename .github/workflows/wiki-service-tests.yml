name: Wiki Service - Tests

on:
  push:
    branches:
      - main
      - feature/**
  pull_request:
    branches:
      - main

jobs:
  tests:
    name: Run Wiki Service test suite
    runs-on: ubuntu-latest

    defaults:
      run:
        shell: bash
        working-directory: services/wiki

    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: Le555ecure
          POSTGRES_DB: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt', 'services/wiki/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies (root)
        working-directory: .
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Wiki service extra dependencies
        run: |
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

      - name: Install pre-commit
        run: |
          pip install pre-commit

      - name: Run pre-commit checks
        run: |
          pre-commit run --all-files

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Create test database
        env:
          PGHOST: localhost
          PGPORT: 5432
          PGUSER: postgres
          PGPASSWORD: Le555ecure
        run: |
          # Wait for PostgreSQL to be ready
          until pg_isready -h localhost -p 5432 -U postgres; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done
          # Create test database (if it doesn't exist)
          psql -h localhost -U postgres -tc "SELECT 1 FROM pg_database WHERE datname = 'arcadium_testing_wiki'" | grep -q 1 || \
          psql -h localhost -U postgres -c "CREATE DATABASE arcadium_testing_wiki"

      - name: Install pytest reporting tools
        run: |
          pip install pytest-html pytest-json-report pytest-json-report-append

      - name: Run pytest for Wiki Service
        env:
          FLASK_ENV: testing
          TEST_DATABASE_URL: postgresql://postgres:Le555ecure@localhost:5432/arcadium_testing_wiki
        continue-on-error: true
        run: |
          # Run tests with concise output during execution
          # Full details will be in reports
          pytest \
            --tb=line \
            --maxfail=50 \
            --junit-xml=test-results.xml \
            --html=test-report.html \
            --self-contained-html \
            --json-report \
            --json-report-file=test-report.json \
            -q \
            --durations=10 \
            --color=yes \
            -ra

      - name: Generate failure summary
        if: always()
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          from collections import defaultdict

          try:
              with open('test-report.json', 'r') as f:
                  report = json.load(f)

              summary = report.get('summary', {})
              total = summary.get('total', 0)
              passed = summary.get('passed', 0)
              failed = summary.get('failed', 0)
              skipped = summary.get('skipped', 0)
              error = summary.get('error', 0)

              # Write comprehensive summary
              with open('test-summary.txt', 'w') as f:
                  f.write("=" * 80 + "\n")
                  f.write("TEST EXECUTION SUMMARY\n")
                  f.write("=" * 80 + "\n\n")
                  f.write(f"Total tests: {total}\n")
                  f.write(f"âœ… Passed: {passed}\n")
                  f.write(f"âŒ Failed: {failed}\n")
                  f.write(f"â­ï¸  Skipped: {skipped}\n")
                  if error > 0:
                      f.write(f"âš ï¸  Errors: {error}\n")
                  f.write("\n")

              # Generate failures-only file for easy copying
              if failed > 0:
                  failed_tests = [t for t in report.get('tests', []) if t.get('outcome') == 'failed']

                  # Group by test file
                  failures_by_file = defaultdict(list)
                  for test in failed_tests:
                      nodeid = test.get('nodeid', 'unknown')
                      # Extract file path (everything before ::)
                      file_path = nodeid.split('::')[0] if '::' in nodeid else nodeid
                      failures_by_file[file_path].append(test)

                  with open('test-failures-only.txt', 'w') as f:
                      f.write("=" * 80 + "\n")
                      f.write(f"FAILED TESTS ({failed} total)\n")
                      f.write("=" * 80 + "\n\n")

                      for file_path, tests in sorted(failures_by_file.items()):
                          f.write(f"\nğŸ“ {file_path}\n")
                          f.write("-" * 80 + "\n")

                          for test in tests:
                              nodeid = test.get('nodeid', 'unknown')
                              call = test.get('call', {})
                              longrepr = call.get('longrepr', 'No error details')

                              # Extract test name
                              test_name = nodeid.split('::')[-1] if '::' in nodeid else nodeid

                              f.write(f"\nâŒ {test_name}\n")
                              f.write(f"   Full path: {nodeid}\n")
                              f.write("\n")

                              # Show full error (not truncated)
                              f.write("Error Details:\n")
                              f.write("-" * 40 + "\n")
                              f.write(longrepr)
                              f.write("\n")
                              f.write("-" * 40 + "\n")
                              f.write("\n")

                  # Also create a compact version
                  with open('test-failures-compact.txt', 'w') as f:
                      f.write("=" * 80 + "\n")
                      f.write(f"FAILED TESTS - COMPACT ({failed} total)\n")
                      f.write("=" * 80 + "\n\n")

                      for test in failed_tests:
                          nodeid = test.get('nodeid', 'unknown')
                          call = test.get('call', {})
                          longrepr = call.get('longrepr', 'No error details')

                          # Extract just the essential error message (first few lines)
                          error_lines = longrepr.split('\n')
                          # Find the actual error (usually after some traceback)
                          error_msg = ""
                          for i, line in enumerate(error_lines):
                              if any(keyword in line for keyword in ['Error:', 'AssertionError', 'Exception:', 'Failed:', 'FAILED']):
                                  # Get next 10 lines as context
                                  error_msg = '\n'.join(error_lines[max(0, i-2):i+10])
                                  break

                          if not error_msg:
                              error_msg = '\n'.join(error_lines[:15])

                          f.write(f"âŒ {nodeid}\n")
                          f.write(f"{error_msg}\n")
                          f.write("\n" + "-" * 80 + "\n\n")

              else:
                  # No failures - create empty files
                  with open('test-failures-only.txt', 'w') as f:
                      f.write("No test failures.\n")
                  with open('test-failures-compact.txt', 'w') as f:
                      f.write("No test failures.\n")

          except Exception as e:
              print(f"Error generating summary: {e}", file=sys.stderr)
              import traceback
              traceback.print_exc()
              sys.exit(1)
          PYTHON_SCRIPT

          except Exception as e:
              print(f"Error generating summary: {e}", file=sys.stderr)
              sys.exit(1)
          PYTHON_SCRIPT

      - name: Display test summary
        if: always()
        run: |
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "TEST EXECUTION SUMMARY"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""

          if [ -f test-summary.txt ]; then
            cat test-summary.txt
          else
            echo "âš ï¸  Summary file not found"
          fi

          # Show failures in compact format
          if [ -f test-failures-compact.txt ]; then
            echo ""
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo "FAILED TESTS (COMPACT - Easy to Copy)"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo ""
            cat test-failures-compact.txt
          fi

          # Show pass/fail status
          if [ -f test-report.json ]; then
            python3 << 'PYTHON_SCRIPT'
          import json
          import sys

          try:
              with open('test-report.json', 'r') as f:
                  report = json.load(f)
              summary = report.get('summary', {})
              failed = summary.get('failed', 0)
              total = summary.get('total', 0)
              passed = summary.get('passed', 0)

              if failed > 0:
                  print(f"\nâŒ {failed} of {total} test(s) failed ({passed} passed)")
                  print(f"\nğŸ“‹ Full failure details available in:")
                  print(f"   - test-failures-only.txt (detailed, grouped by file)")
                  print(f"   - test-failures-compact.txt (compact, easy to copy)")
                  print(f"   - test-report.html (interactive HTML report)")
                  sys.exit(1)
              else:
                  print(f"\nâœ… All {total} tests passed")
          except Exception as e:
              print(f"Error reading report: {e}", file=sys.stderr)
              sys.exit(1)
          PYTHON_SCRIPT
          fi

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: wiki-test-results
          path: |
            services/wiki/test-results.xml
            services/wiki/test-report.html
            services/wiki/test-report.json
            services/wiki/test-summary.txt
            services/wiki/test-failures-only.txt
            services/wiki/test-failures-compact.txt
          retention-days: 7
          if-no-files-found: ignore

      - name: Publish test results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: services/wiki/test-results.xml
          check_name: Wiki Service Test Results
          comment_mode: changes
          report_individual_runs: true
